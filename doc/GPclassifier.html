<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="Youjie_Zeng_20088" />


<title>GPclassifier_20088</title>

<script>// Hide empty <a> tag within highlighted CodeBlock for screen reader accessibility (see https://github.com/jgm/pandoc/issues/6352#issuecomment-626106786) -->
// v0.0.1
// Written by JooYoung Seo (jooyoung@psu.edu) and Atsushi Yasumoto on June 1st, 2020.

document.addEventListener('DOMContentLoaded', function() {
  const codeList = document.getElementsByClassName("sourceCode");
  for (var i = 0; i < codeList.length; i++) {
    var linkList = codeList[i].getElementsByTagName('a');
    for (var j = 0; j < linkList.length; j++) {
      if (linkList[j].innerHTML === "") {
        linkList[j].setAttribute('aria-hidden', 'true');
      }
    }
  }
});
</script>
<style type="text/css">
a.anchor-section {margin-left: 10px; visibility: hidden; color: inherit;}
a.anchor-section::before {content: '#';}
.hasAnchor:hover a.anchor-section {visibility: visible;}
</style>
<script>// Anchor sections v1.0 written by Atsushi Yasumoto on Oct 3rd, 2020.
document.addEventListener('DOMContentLoaded', function() {
  // Do nothing if AnchorJS is used
  if (typeof window.anchors === 'object' && anchors.hasOwnProperty('hasAnchorJSLink')) {
    return;
  }

  const h = document.querySelectorAll('h1, h2, h3, h4, h5, h6');

  // Do nothing if sections are already anchored
  if (Array.from(h).some(x => x.classList.contains('hasAnchor'))) {
    return null;
  }

  // Use section id when pandoc runs with --section-divs
  const section_id = function(x) {
    return ((x.classList.contains('section') || (x.tagName === 'SECTION'))
            ? x.id : '');
  };

  // Add anchors
  h.forEach(function(x) {
    const id = x.id || section_id(x.parentElement);
    if (id === '') {
      return null;
    }
    let anchor = document.createElement('a');
    anchor.href = '#' + id;
    anchor.classList = ['anchor-section'];
    x.classList.add('hasAnchor');
    x.appendChild(anchor);
  });
});
</script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>





<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">GPclassifier_20088</h1>
<h4 class="author">Youjie_Zeng_20088</h4>
<h4 class="date">2020/12/20</h4>



<hr />
<div id="model-introduction" class="section level2">
<h2>Model introduction</h2>
<p>Supposed we have the observed data set <span class="math inline">\(\mathcal{D} = \{y_i,\boldsymbol{x}_i\}\)</span> in which the response variable <span class="math inline">\(y\)</span> is binary data, i.e. <span class="math inline">\(y=0\)</span> or <span class="math inline">\(1\)</span>, and the covariate <span class="math inline">\(\boldsymbol{x}\)</span> associate with the response <span class="math inline">\(y\)</span> is a multi-dimensional functional variable. Then the clustering problem is to estimate the response probability <span class="math inline">\(P(y =1|\boldsymbol{x})=\pi(\boldsymbol{x})\)</span>. A <span class="math inline">\(GP\ \ binary\ \ regression\ \ model\)</span> is illustrated as follow. The binary functional response variable has a binomial distribution with parameters <span class="math inline">\(1\)</span> and <span class="math inline">\(\pi(t)\)</span>, i.e. <span class="math display">\[\begin{align}
    y(t) |\pi(\boldsymbol{x}(t)) \sim Bin (1,\pi(\boldsymbol{x}(t)))
\end{align}\]</span> If we use a logit link, <span class="math inline">\(logit[\pi(\boldsymbol{x}(t))] = f(\boldsymbol{x}(t))\)</span>, then the latent variable <span class="math inline">\(f(\boldsymbol{x}(t))\)</span> can be modelled by the GPR model, i.e. <span class="math display">\[\begin{align}
    f(\boldsymbol{x}(t)) \sim GPR (0,k(\cdot,\cdot;\boldsymbol{\theta})|\boldsymbol{x}(t))
\end{align}\]</span> where <span class="math inline">\(\boldsymbol{x}(t)\)</span> are <span class="math inline">\(Q\)</span>-dimensional function covariates and <span class="math inline">\(k(\cdot,\cdot;\boldsymbol{\theta})\)</span> is a covariance kernel. Thus, given the observed data <span class="math inline">\(\mathcal{D}\)</span> at data points $ t =1,…,n$, we have write the discrete form of the model as <span class="math display">\[\begin{align}
    y_i | \pi_i \sim i.i.d \ \  Bin (1,\pi_i),\ \ i =1,...,n.
\end{align}\]</span> Using the logit link, the latent variable <span class="math inline">\(f_i\)</span> is defined by <span class="math inline">\(f_i = f(\boldsymbol{x}_i)=logit[\pi(\boldsymbol{x}_i)]\)</span>. From the Gaussian process binary regression model, we have <span class="math display">\[ \boldsymbol{f}  = (f_1,...,f_n) \sim N(0,\boldsymbol{K}(\boldsymbol{\theta})),\]</span> where <span class="math inline">\(\boldsymbol{K}\)</span> is defined by a kernel covaiance <span class="math inline">\(k(\cdot,\cdot;\boldsymbol{\theta})\)</span> ane the <span class="math inline">\((i,j)\)</span>-th element of <span class="math inline">\(\boldsymbol{K}\)</span> is given by <span class="math inline">\(Cov(f_i,f_j) = k(\boldsymbol{x}_i,\boldsymbol{x}_j;\boldsymbol{\theta})\)</span>. Then the density fucntion of <span class="math inline">\(\boldsymbol{y}\)</span> given $ $ can be written by <span class="math display">\[\begin{align}
    p(\boldsymbol{y}|\boldsymbol{f}) = \prod_{i=1}^n \pi_i^{y_i}(1-\pi_i)^{1-y_i} = \prod_{i=1}^n \frac{\exp(f_i y_i)}{1 + \exp(f_i)}.
\end{align}\]</span> Thus, the parameters <span class="math inline">\(\boldsymbol{\theta}\)</span> can be estimated by maximizing the marginal distribution of <span class="math inline">\(\boldsymbol{y}\)</span> which is given by <span class="math display">\[\begin{align}
    p(\boldsymbol{y}|\boldsymbol{x},\boldsymbol{\theta}) = \int p(\boldsymbol{y}|\boldsymbol{f}) p(\boldsymbol{f}|\boldsymbol{x},\boldsymbol{\theta}) d \boldsymbol{f}
\end{align}\]</span> The above integral has not a closed form since response variable <span class="math inline">\(y\)</span> has a exponential family distribution. We consider the Laplace approximation.</p>
<p>We consider alternatively the marginal log-likelihood <span class="math display">\[\begin{align}\label{loglikelihood}
    l (\boldsymbol{\theta}) = \log(p(\boldsymbol{y}|\boldsymbol{x},\boldsymbol{\theta})) = \log \left( \int \exp(\gamma(\boldsymbol{f})) d \boldsymbol{f} \right)
\end{align}\]</span> where <span class="math display">\[\begin{align}
\gamma (\boldsymbol{f}) = \log(p(\boldsymbol{y}|\boldsymbol{f})) + \log (p(\boldsymbol{f}|\boldsymbol{x},\boldsymbol{\theta})). \label{gamma}
\end{align}\]</span> Meanwhile, We have <span class="math display">\[\begin{align}
    \frac{\partial \gamma(\boldsymbol{f})}{\partial \boldsymbol{f}} &amp;= \boldsymbol{y} - \boldsymbol{\pi} - \boldsymbol{K}_n^{-1} \boldsymbol{f}, \label{firstD}\\
    \frac{\partial \gamma(\boldsymbol{f})}{\partial \boldsymbol{f} \partial \boldsymbol{f}^T} &amp;= -\boldsymbol{A} - \boldsymbol{K}_n^{-1},\label{secondD}
\end{align}\]</span> where <span class="math inline">\(\boldsymbol{\pi} = (\pi_1,...,\pi_n)^T\)</span> and <span class="math inline">\(\boldsymbol{A}= \text{diag}(\pi_1(1-\pi_1),...,\pi_n(1-\pi_n))\)</span>. By Laplace method, the integral in () can be approximated by <span class="math display">\[\begin{align}\label{LaplaceApprox}
    l(\boldsymbol{\theta}) &amp;= \log \int \exp(\gamma(\boldsymbol{f})) d\boldsymbol{f} \\
    &amp;\approx \gamma(\hat{\boldsymbol{f}}) + \frac{n}{2} \log(2\pi) -\frac{1}{2} \log| \boldsymbol{K}_n^{-1} +\boldsymbol{A}| \\
    &amp;= \log(p(\boldsymbol{y}|\hat{\boldsymbol{f}})) + \log(p(\hat{\boldsymbol{f}}|\boldsymbol{x},\boldsymbol{\theta})) + \frac{n}{2}\log(2\pi) -\frac{1}{2} \log| \boldsymbol{K}_n^{-1} + \boldsymbol{A}| \\
    &amp; = -\frac{1}{2}\log| \boldsymbol{K}_n| - \frac{1}{2}\hat{\boldsymbol{f}}^T \boldsymbol{K_n}^{-1} \hat{\boldsymbol{f}} - \frac{1}{2} \log| \boldsymbol{K}^{-1}_n + \boldsymbol{A}| + C
\end{align}\]</span> where <span class="math inline">\(\hat{\boldsymbol{f}}\)</span> is the maximizer of $ ()$ and <span class="math inline">\(C\)</span> is the constant independent with <span class="math inline">\(\boldsymbol{\theta}\)</span>. We can also obtain the first two derivatives of $ l()$ in () as follow, <span class="math display">\[\begin{align}
    \frac{\partial l(\boldsymbol{\theta})}{\partial \theta_j} &amp;= -\frac{1}{2} \text{tr} \left[ \left( \boldsymbol{K}^{-1}_n +(\boldsymbol{K}^{-1}_n + \boldsymbol{A})^{-1}\right) \frac{\partial \boldsymbol{K}_n}{\partial \theta_j}\right] + \frac{1}{2} \boldsymbol{\hat{f}}^T \boldsymbol{K}^{-1}_n \frac{\partial \boldsymbol{K}_n}{\partial \theta_j}  \boldsymbol{K}^{-1}_n \boldsymbol{\hat{f}}\\
    &amp;= \frac{1}{2} \text{tr}\left[\left(\boldsymbol{\alpha} \boldsymbol{\alpha}^T - \left( \boldsymbol{K}^{-1}_n +(\boldsymbol{K}^{-1}_n + \boldsymbol{A})^{-1}\right) \right) \frac{\partial \boldsymbol{K}_n}{\partial \theta_j} \right] \label{Dloglike}
\end{align}\]</span> where <span class="math inline">\(\boldsymbol{\alpha} = \boldsymbol{K}^{-1}_n \boldsymbol{\hat{f}}\)</span>. In addition, the second derivatives can be calculated accordingly, and we list the formula here, <span class="math display">\[\begin{align}
    \frac{\partial^2 l(\boldsymbol{\theta})}{\partial\theta_i \partial \theta_j} = &amp; \frac{1}{2}\text{tr}\left[\left(\boldsymbol{\alpha} \boldsymbol{\alpha}^T - \left( \boldsymbol{K}^{-1}_n +(\boldsymbol{K}^{-1}_n + \boldsymbol{A})^{-1}\right) \right) \frac{\partial^2 \boldsymbol{K}_n}{\partial \theta_i \partial \theta_j}\right] - \frac{1}{2} \text{tr}\left[ 2 \boldsymbol{\alpha}\boldsymbol{\alpha}^T \frac{\partial \boldsymbol{K}_n}{\partial \theta_i}\boldsymbol{K}^{-1}_n \frac{\partial \boldsymbol{K}_n}{\partial \theta_j}\right]\\
    &amp; +\frac{1}{2} \text{tr}\left[ \boldsymbol{K}^{-1}_n\frac{\partial \boldsymbol{K}_n}{\partial \theta_i} \boldsymbol{K}^{-1}_n \frac{\partial \boldsymbol{K}_n}{\partial \theta_j}\right] + \frac{1}{2} \text{tr}\left[ (\boldsymbol{K}^{-1}_n+\boldsymbol{A})^{-1}\frac{\partial \boldsymbol{K}_n}{\partial \theta_i} (\boldsymbol{K}^{-1}_n+\boldsymbol{A})^{-1} \frac{\partial \boldsymbol{K}_n}{\partial \theta_j}\right]. \label{DDloglike}
\end{align}\]</span></p>
<p>Empirical Bayesian estimates of <span class="math inline">\(\boldsymbol{\theta}\)</span> can therefore be calculated by maximizing approximated log-likelihood. However, <span class="math inline">\(\gamma(\boldsymbol{f})\)</span> depends on <span class="math inline">\(\boldsymbol{\theta}\)</span>. So we need an iterative method, starting with initial values of <span class="math inline">\(\boldsymbol{\theta}\)</span> and then updating <span class="math inline">\(\boldsymbol{f}\)</span> and $ $ in turn until both converge.</p>
<p>(Empirical Bayesian learning for the GP binary regression model). Each iteration include the following two steps:</p>
<p>Given <span class="math inline">\(\boldsymbol{\theta}\)</span>, update <span class="math inline">\(\boldsymbol{f}\)</span> by maximizeing <span class="math inline">\(\gamma(\boldsymbol{f})\)</span></p>
<p>Given <span class="math inline">\(\boldsymbol{f}\)</span>, update <span class="math inline">\(\boldsymbol{\theta}\)</span> by maximizing <span class="math inline">\(l(\boldsymbol{\theta})\)</span>.</p>
<p>After that we obtained <span class="math inline">\(\hat{\boldsymbol{\theta}}\)</span> and <span class="math inline">\(\hat{\boldsymbol{f}}\)</span> from the above Algorithm, given a new input point <span class="math inline">\(\boldsymbol{x}^*\)</span> and let <span class="math inline">\(f^* = f(\boldsymbol{x}^*)\)</span>, the simple estimates of the predictive mean and variance of <span class="math inline">\(f^*\)</span> are given by <span class="math display">\[\begin{align}
    E(f^*|\mathcal{D},\boldsymbol{x}) &amp;\approx \boldsymbol{k}^* \boldsymbol{K}_n^{-1} (\boldsymbol{\hat{\theta}}) \hat{\boldsymbol{f}}, \\
    Var(f^*|\mathcal{D},\boldsymbol{x}) &amp;\approx k(\boldsymbol{x}^*,\boldsymbol{x}^*) - \boldsymbol{k}^{*T} (\boldsymbol{A}^{-1} + \boldsymbol{K}_n)^{-1}\boldsymbol{k}^* 
\end{align}\]</span> where <span class="math inline">\(\boldsymbol{k}^* = (k(\boldsymbol{x}^*,\boldsymbol{x}_1),...,k(\boldsymbol{x}^*,\boldsymbol{x}_n))^T\)</span>. Once <span class="math inline">\(\hat{f}^*\)</span> is calculated, we can estimate the value of <span class="math inline">\(\pi^*\)</span>, the predictive probability. Consequently, the prediction of <span class="math inline">\(y^*\)</span> takes either the value of <span class="math inline">\(1\)</span> if <span class="math inline">\(\pi^*\geq 0.5\)</span> or <span class="math inline">\(0\)</span> either.</p>
</div>
<div id="fit.gpbinary" class="section level2">
<h2><span class="math inline">\(fit.GPbinary()\)</span></h2>
<p>The main function to estimate the kernel hyper-parameters by using maximizing the marginal log-likelihood and Laplace approximation iteratively.</p>
</div>
<div id="gp.predict" class="section level2">
<h2><span class="math inline">\(GP.predict()\)</span></h2>
<p>When model are trained, i.e., hyper-parameters are estimated by using <span class="math inline">\(fit.GPbinary()\)</span>, given a new input <span class="math inline">\(\boldsymbol{x^*}=(x_1^*,x_2^*)\)</span>, this function gives a prediction at <span class="math inline">\(\boldsymbol{x^*}\)</span>, say <span class="math inline">\(y(x^*)\)</span>.</p>
</div>
<div id="y.loglike" class="section level2">
<h2><span class="math inline">\(y.loglike()\)</span></h2>
<p>Calculate the minus marginal log-likelihood with Laplace approximation.</p>
</div>
<div id="y.dloglike" class="section level2">
<h2><span class="math inline">\(y.Dloglike()\)</span></h2>
<p>Calculate the gredient for minus marginal log-likelihood for obtaining the maximizer, i.e., the MLE of hyper-parameters.</p>
</div>
<div id="gamma.f" class="section level2">
<h2><span class="math inline">\(gamma.f()\)</span></h2>
<p>Calculate the Laplace approximation.</p>
</div>
<div id="dgamma.f" class="section level2">
<h2><span class="math inline">\(Dgamma.f()\)</span></h2>
<p>Calculate the gradient for Laplace approximation.</p>
</div>
<div id="cov.func" class="section level2">
<h2><span class="math inline">\(cov.func()\)</span></h2>
<p>Calculate the covariance matrix.</p>
</div>
<div id="mymatinv" class="section level2">
<h2><span class="math inline">\(mymatinv()\)</span></h2>
<p>Calculate the inverse of a symmetry matrix and add a jitter to aviod singular case.</p>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
